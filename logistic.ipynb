{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('loan_classification.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Getting the summary of the DataFrame\n",
    "data_info = pd.DataFrame({\n",
    "    'Data Type': data.dtypes,\n",
    "    'Unique Values': data.nunique(),\n",
    "    'Missing Values': data.isnull().sum(),\n",
    "    'First Record': data.iloc[0]\n",
    "})\n",
    "\n",
    "print(data_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecting features and target\n",
    "X = data.drop('loan_status', axis=1)\n",
    "y = data['loan_status'].apply(lambda x: 1 if x == 'Approved' else 0)\n",
    "\n",
    "# Defining the columns that need encoding and scaling\n",
    "categorical_features = ['gender', 'occupation', 'education_level', 'marital_status']\n",
    "numeric_features = ['age', 'income', 'credit_score']\n",
    "\n",
    "# Creating transformers for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Applying the transformations\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(logreg, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Average cross-validation score:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning with GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter grid definition\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Grid search initialization\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)\n",
    "\n",
    "# Fitting grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEATURE SELECTION - Univariate\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "\n",
    "# Applying Variance Threshold to remove features with zero variance\n",
    "var_thresh = VarianceThreshold(threshold=0)\n",
    "X_train_var = var_thresh.fit_transform(X_train)\n",
    "\n",
    "# Reapply SelectKBest on the filtered dataset\n",
    "selector_kbest = SelectKBest(f_classif, k=10)\n",
    "X_train_kbest = selector_kbest.fit_transform(X_train_var, y_train)\n",
    "\n",
    "# Extract feature names from the preprocessor that are not filtered out by VarianceThreshold\n",
    "full_features_mask = var_thresh.get_support()  # This is the mask after applying VarianceThreshold\n",
    "feature_names = preprocessor.transformers_[1][1].get_feature_names_out()\n",
    "adjusted_feature_names = [name for name, var in zip(feature_names, full_features_mask) if var]\n",
    "\n",
    "# Apply SelectKBest mask to the adjusted feature names\n",
    "selected_features_mask = selector_kbest.get_support()  # This mask applies to the variance-filtered dataset\n",
    "final_selected_features = [name for name, select in zip(adjusted_feature_names, selected_features_mask) if select]\n",
    "\n",
    "print(\"Features selected by Univariate Selection:\", final_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FEATURE SELECTION - Model based\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Applying model-based selection\n",
    "selector_model = SelectFromModel(LogisticRegression(max_iter=1000))\n",
    "X_train_model = selector_model.fit_transform(X_train, y_train)\n",
    "\n",
    "# Identifying which features were selected\n",
    "mask_model = selector_model.get_support()\n",
    "selected_columns_model = [col for col, selected in zip(preprocessor.transformers_[1][1].get_feature_names_out(), mask_model) if selected]\n",
    "print(\"Features selected by Model-based Selection:\", selected_columns_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FEATURE SELECTION - Recursive Feature Elimination (RFE)\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Applying RFE\n",
    "rfe = RFE(estimator=LogisticRegression(max_iter=1000), n_features_to_select=10)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "# Identifying which features were selected\n",
    "mask_rfe = rfe.support_\n",
    "selected_columns_rfe = [col for col, selected in zip(preprocessor.transformers_[1][1].get_feature_names_out(), mask_rfe) if selected]\n",
    "print(\"Features selected by RFE:\", selected_columns_rfe)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
